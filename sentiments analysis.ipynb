{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm as tqdm\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"A1_dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apunctuations = string.punctuation\n",
    "def remove_punct(sent):\n",
    "    x=\"\".join([letter for letter in sent if letter not in punctuations])\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_spaces(sent):\n",
    "    x = re.sub(r'\\s+',' ',sent)#will remove extra spaces\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(sent):\n",
    "    tokens = word_tokenize(sent)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    good_words = []\n",
    "    for word in tokens:\n",
    "        match = re.match('(?!no)\\w{1,2}\\\\b',word+' ')\n",
    "        x = bool(match)\n",
    "        if x is not True :\n",
    "            good_words.append(word)\n",
    "    row = ' '.join(good_words)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_html(sent):\n",
    "    pat = re.compile(\"@[a-zA-Z0-9_]+\")#atleast one character should be present after @ and valid username contains alphabets,digits and _\n",
    "    cl_pat = re.sub(pat, ' ', sent)\n",
    "    pat = re.compile(\"www.\\S+\")\n",
    "    cl_pat = re.sub(pat, ' ', cl_pat)\n",
    "    pat = re.compile(\"<.*?>\")#html\n",
    "    cl_pat = re.sub(pat, ' ', cl_pat)\n",
    "    pat = re.compile(\"https?://\\S+\")#url\n",
    "    cl_pat = re.sub(pat, ' ', cl_pat)\n",
    "    return str(cl_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(sent):\n",
    "    corr = TextBlob(sent)\n",
    "    sent_corr = corr.correct()\n",
    "    return str(sent_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(row):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_words=[]\n",
    "    for word in row.split(\" \"):\n",
    "\n",
    "        lemm_words.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    lemmed = ' '.join(lemm_words)\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(row):\n",
    "    row = remove_url_html(row)\n",
    "    row = remove_punct(row)\n",
    "    row = remove_white_spaces(row)\n",
    "    words = tokenization(row)\n",
    "    row = remove_stop_words(words)\n",
    "    row = spelling_correction(row)\n",
    "    row = lemmatizing(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_on_one_sentence(sentence):\n",
    "    sentence = remove_url_html(sentence)\n",
    "    print(\"after url-html removal :\",sentence)\n",
    "    sentence = remove_punct(sentence)\n",
    "    print(\"after punctuation removal :\",sentence)\n",
    "    sentence = remove_white_spaces(sentence)\n",
    "    print(\"after white space removal :\",sentence)\n",
    "    words = tokenization(sentence)\n",
    "    print(\"after tokenization :\",words)\n",
    "    sentence = remove_stop_words(words)\n",
    "    print(\"after stop words removal :\",sentence)\n",
    "    sentence = spelling_correction(sentence)\n",
    "    print(\"after spelling  correction :\",sentence)\n",
    "    sentence = lemmatizing(sentence)\n",
    "    print(\"after lemmatizing :\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.loc[322]) #sentence belonging to class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data.loc[322,'TEXT']\n",
    "print(\"Before preprocessing \\n\",sentence)\n",
    "text_preprocessing_on_one_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.loc[865]) #sentence belonging to class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data.loc[865,'TEXT']\n",
    "print(\"Before preprocessing \\n\",sentence)\n",
    "text_preprocessing_on_one_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(data.index):\n",
    "    data.loc[i,'TEXT'] = preprocessing_pipeline(data.loc[i,'TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pos = \" \".join(data.loc[i,'TEXT'] for i in data.index if data.loc[i,'LABEL']==1)\n",
    "text_neg = \" \".join(data.loc[i,'TEXT'] for i in data.index if data.loc[i,'LABEL']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud().generate(text_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud().generate(text_neg)\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_features = 2000000 #maximum words in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=Max_features,\n",
    "                               output_sequence_length=350,\n",
    "                               output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(data['TEXT'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = vectorizer(data['TEXT'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= data['LABEL'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model11=tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(16,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(8,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model11.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer='Adam',\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "#create a learning rate callback\n",
    "lr_scheduler=tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))\n",
    "\n",
    "history11=model11.fit(vectorized_text,labels,epochs=100,callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history11.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model1=tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(16,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(8,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model1.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "#create a learning rate callback\n",
    "lr_scheduler=tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))\n",
    "\n",
    "history1=model11.fit(vectorized_text,labels,epochs=100,callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model11.predict(vectorized_text[500:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels[500:1500],tf.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MCSHBAP - map, chache, shuffle, batch, prefetch  from_tensor_slices, list_file\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, labels))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(160000)\n",
    "dataset = dataset.batch(64)\n",
    "dataset = dataset.prefetch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.take(int(len(dataset)*.7))\n",
    "test = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding,Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Create the embedding layer\n",
    "model.add(Embedding(Max_features+1, 32))\n",
    "# Bidirectional LSTM Layer\n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "# Feature extractor Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Final layer\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train, epochs=20, validation_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss Function')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Validation'], loc='upper left')\n",
    "plt.show()input='I am not unhappy person'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/content/analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input='I am not unhappy person'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before preprocessing \\n\",input)\n",
    "text_preprocessing_on_one_sentence(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(vectorized_text)\n",
    "k=res.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_name=['Negative Comment',' Positive Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_name[int(k[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis model\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    result = sentiment_analysis(text)[0]\n",
    "    label = result['label']\n",
    "\n",
    "    return f'Sentiment: {label}'\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(fn=analyze_sentiment, inputs=\"text\", outputs=\"text\", title=\"Sentiment Analysis\")\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
